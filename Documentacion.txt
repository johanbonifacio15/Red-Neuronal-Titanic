DOCUMENTACION FINAL - RED NEURONAL TITANIC  
=========================================== 

### 1. DISEÑO DEL SISTEMA  

Arquitectura de la Red: 
- Capa entrada: 15 neuronas (features)  
- Capas ocultas: 10 y 5 neuronas (LeakyReLU)  
- Capa salida: 1 neurona (Sigmoid)  

Preprocesamiento:
- Normalizacion de edad (0-80) y fares (log-transform)  
- One-hot encoding: embarked, titulos (Mr, Mrs, etc.)  
- Imputacion de valores faltantes (media para edad)  

Paralelizacion:
- Se implemento en:  
  * Calculo de errores (deltas)  
  * Actualizacion de pesos  
- Uso de Parallel.For para division de carga  

### 2. RESULTADOS OBTENIDOS 

Metricas principales:
- Precision maxima: 78% (test set)  
- Perdida final (MSE): 0.18 (inicial 0.24)  
- Speedup paralelizacion: 1.89x  

Tiempos de ejecucion en promedio (1000 epocas): 
- Paralelo: 2.78 seg (359.57 epocas/seg)  
- Secuencial: 5.25 seg (190.64 epocas/seg)  

Ejemplo de salida del programa:
Epoca 700: Precision = 75.98% | Perdida = 0.1893  
Predicciones:  
- Real: Sobrevivio, Pred: Sobrevive (0.7234) [✓]  
- Real: No sobrevivio, Pred: No sobrevive (0.4211) [✓]  

### 3. CONCLUSIONES

Lo implementado exitosamente:  
- Red neuronal funcional desde cero en C#  
- Paralelizacion efectiva (1.89x mas rapido)  
- Sistema completo: preprocesamiento, entrenamiento y evaluacion  

Limitaciones encontradas:
- Overhead en datasets pequeños  
- Precision limitada por naturaleza historica de los datos  

Mejoras posibles: 
- Regularizacion (L2/dropout)  
- Ajuste dinamico de learning rate  
- Validacion cruzada  

Aporte clave: 
- Ejemplo practico de paralelizacion en ML  
- Implementacion didactica de backpropagation  

### 4. REFERENCIAS  
1. Microsoft Docs: Parallel.For: https://docs.microsoft.com/en-us/dotnet/api/system.threading.tasks.parallel.for
2. CS231n: Neural Networks Basics: https://cs231n.github.io/neural-networks-1/
3. TPL Best Practices: https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/task-parallel-library-tpl

